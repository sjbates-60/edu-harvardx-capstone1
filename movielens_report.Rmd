---
title: "Modelling Movie Ratings"
subtitle: "Report for HarvardX PH125.9x: Data Science: Capstone"
author: "Samuel Bates"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: pdf_document
geometry: bottom=1.1in
editor_options: 
  markdown: 
    wrap: 80
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_knit$set(eval.after = "fig.cap")

require(tidyverse)
require(lubridate)
require(kableExtra)

# The following file contains the ratings and movies tables.
load("ml-10M100K/movielens_report.RData")

nongenre_results <- readRDS("nongenre_results.rds")
combined_results <- readRDS("combined_results.RDS")
```

# Overview

In this report, I describe my exploration of a dataset of movie ratings made by
many people over the course of fourteen years, and my construction of several
linear models for predicting ratings. The ratings were taken from the MovieLens
website and made available by Grouplens Research.[^1] The models were
constructed from and tested on a training set containing 90% of the ratings. The
remaining 10% of the ratings were held out in a validation set, which was not
used until I had selected a final model among those I constructed. I report how
well each model performed on the training set, and how well the final model
succeeded in predicting the ratings in the validation set.

[^1]: https://grouplens.org/datasets/movielens

# Exploration

The dataset contains 10,000,054 ratings of movies by people (hereinafter
referred to as *users*). There are 10,677 different movies rated and 69,878
different users. The ratings were taken between January 9, 1995 and January 5,
2009. The dataset was provided as two files:

-   `ratings.dat`, containing 10,000,054 observations, each with 4 variables:
    `userId`, `movieId`, `rating` (a number between 0 and 5), and `timestamp`;
-   `movies.dat`, containing 10,681 observations, each with 3 variables:
    `movieId`, `title` (which contains the movie title and the year it was
    released), and `genres` (which contains a list of genres separated by `|`
    symbols).

Tables 1 and 2 show the results of counting how many movies each user rated.
Half of all users rated 69 or fewer movies, and 9 out of 10 rated 335 or fewer
movies. Every user rated at least 20 movies, and roughly 1% of the users rated
more than 1000 movies.

```{r user-stats}
percents <- ratings %>%
  group_by(userId) %>% 
  summarize(num_ratings = n()) %>% 
  pull(num_ratings) %>%
  quantile(probs = seq(0, 1, .1))
knitr::kable(t(percents), booktabs = T,
             caption = "Number of movies rated by users")

high_percents <- ratings %>% 
  group_by(userId) %>% 
  summarize(num_ratings = n()) %>% 
  pull(num_ratings) %>%
  quantile(probs = seq(.9, 1, .01))
knitr::kable(t(high_percents), booktabs = T,
             caption = "Number of movies rated by high-volume users")

rm(percents, high_percents)
```

I present three plots to motivate the use of the different variables in
modelling ratings. Figure 1 shows that there is a positive correlation between a
movie's average rating and the number of times it was rated. This makes
intuitive sense; more people will tend to go to movies that they hear have good
reviews. Figure 2 shows that there is a negative correlation between a user's
average rating and the number of movies rated by that user. One can imagine that
such a user is harder to impress, having seen so many movies.

```{r movie-correlation, fig.height=1.8, fig.cap=movie_caption}
movie_stats <- ratings %>% group_by(movieId)

mean_movie_ratings <- movie_stats %>%
  summarize(num_ratings = n(), mean_rating = mean(rating))

movie_cor <- cor(mean_movie_ratings$num_ratings, 
                 mean_movie_ratings$mean_rating)
movie_caption <-  sprintf("A movie's average rating increases as the number of ratings increases (correlation is %.4f).", movie_cor)

rm(movie_stats, movie_cor)

mean_movie_ratings %>%
  ggplot(aes(num_ratings, mean_rating)) + 
  geom_point(alpha = 0.2) + 
  scale_x_log10() + 
  xlab("Number of ratings per movie (log scale)") +
  ylab("Rating")
```

```{r user-correlation, fig.height=1.8, fig.cap=user_caption}
user_stats <- ratings %>% group_by(userId)

mean_user_ratings <- user_stats %>%
  summarize(num_ratings = n(), mean_rating = mean(rating))

user_cor <- cor(mean_user_ratings$num_ratings, mean_user_ratings$mean_rating)
user_caption <-  sprintf("A user's average movie rating decreases as the number of movies rated increases (correlation is %.4f).", user_cor)

rm(user_stats, user_cor)

mean_user_ratings %>%
  ggplot(aes(num_ratings, mean_rating)) + 
  geom_point(alpha = 0.2) + 
  scale_x_log10() +
  xlab("Number of movies rated by user (log scale)") +
  ylab("Rating")
```

Lastly, a movie's ratings may also change over time. Figure 3 shows how the
average rating per month of the movie "Jerry Maguire" decreased over the 14
years after it was released.

```{r rating-vs-time, fig.height=1.8, fig.cap="A movie's rating can change over time."}
ratings %>% 
  filter(movieId == 1393) %>%
  mutate(delay = year(timestamp) - 1996 + month(timestamp) / 13) %>%
  group_by(delay) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(delay, rating)) +
  geom_point() +
  ylim(0, 5) +
  xlab("Number of years after movie release") +
  ylab("Average rating") +
  labs(title = "Jerry Maguire")
```

# Methods and Analysis

## Data Wrangling

The `movies.dat` table has two variables that were transformed in order to be
used in models.

-   The release year is contained in the `title` variable. This was separated
    into a new `year` variable in order to model the effect of time on ratings.

-   The `genres` variable is a list of `|`-separated genres. There are 19
    different genres, plus a "(no genre specified)" option. The variable was
    separated into 20 variables, each named for a genre. A genre variable
    contains a 1 if the movie has the variable name in its `genres` variable,
    and a 0 otherwise.

The `ratings.dat` table has a `timestamp` variable that is a second count from
12:00 am on January 1, 1970. It was converted into a `POSIXct` variable so that
the time between a movie's release year and a rating's creation time can be
calculated.

I created a training set containing 9,000,055 ratings and a validation set
containing 999,999 ratings. All exploration and modelling was done exclusively
on the training set.

My exploration was done in two phases. In the first phase, I tested models that
excluded the genres of the movies. In the second phase, I subtracted the best
model's predicted values from the actual values, and used the genres in a
subsequent model.

## Models without Genre

The models without genre that I created were all linear models on some subset of
the variables `userId`, `movieId`, `year`, and `timestamp`.[^2] I included the
user and movie variables in all of them.[^3] Rather than including the timestamp
variable directly in a model, I calculated an age variable. The age is an
approximate measure of how old the movie was when the user rated it. I
calculated it as `(year(timestamp) + month(timestamp) / 13) - year`. I divided
by 13 instead of by 12 so that the integral part of the age would equal the year
of the rating (otherwise, ratings in December would have the integral part equal
to the next year). Another advantage is that every age is non-zero. I chose the
month for the fraction rather than the day or week of the year, as it exhibited
a smoother rate of change.

[^2]: I left out `title` as I could not think of a useful distance function
    between titles.

[^3]: Although the models are not included in the final code, I did create a
    model and generate an RMSE value for each pair of variables. If only one of
    the movie and user variables was in the model, the RMSE was at least .94.

Furthermore, I added regularization to different variables in the models. The
movie variable was always regularized, since there are many movies with 4 or
fewer ratings.

The result of these decisions was a set of eleven models:

-   Two models with movie and user effects: both regularized or only movie
    effects regularized;

-   Four models with movie, user, and year effects: all three regularized and
    user or year or both effects not regularized;

-   Four models with movie, user, and age effects: all three regularized and
    user or age or both effects not regularized;

-   One model with movie, user, year, and age effects, with only movie effects
    regularized. This model was added after seeing the results of the previous
    ten models.

For each model, I calculated an optimal tuning parameter $\lambda$ for the
regularization and then averaged the results from 5-fold cross validation on the
training set. I then chose the model that resulted in the lowest RMSE on the
entire training set.

## Models with Genre

The second phase was to explore the effects of genre after removing the best
estimate from the first phase. I could have created another model using the
`genres` variable directly, each combination of genres being a separate
category. There are two problems with this approach:

-   Genre combinations that share genres would be considered entirely
    separately. For example, a user's rating of a movide with genre
    "Comedy\|Romance" would have no relation to the user's rating of a movie
    with genre "Adventure\|Comedy\|Romance," which we intuitively know is not
    likely.

-   The model would perform no better than a non-genre model for a movie with a
    genre combination previously unrated by a user.

Therefore, I chose a linear combination of individual genre variables for the
model. As described above, I separated the `genres` variable into 20 separate
variables, one for each of the genres in Table 3. For display purposes, I
abbreviated each genre to two characters. Each genre variable has a value of 1
if the genre appears in the movie's `genres` variable and has a value of 0
otherwise.

```{r genres-table}
all_genres <- unique(unlist(sapply(movies$genres,
                                   function(g) strsplit(g, "\\|"))))
abbrevs <- data.frame(genres = all_genres) %>%
  mutate(genres = gsub("([^\\|]{2})[^\\|]+", "\\1", genres)) %>%
  mutate(genres = sub("Sc", "SF", genres)) %>%
  mutate(genres = sub("Fi", "FN", genres)) %>%
  mutate(genres = sub("\\(n", "No", genres))

summary <- data.frame(all_genres[1:5], abbrevs[1:5, 1],
                      all_genres[6:10], abbrevs[6:10, 1],
                      all_genres[11:15], abbrevs[11:15, 1],
                      all_genres[16:20], abbrevs[16:20, 1])
knitr::kable(summary, 
             col.names = c("Genre", "Abbr.",
                           "Genre", "Abbr.",
                           "Genre", "Abbr.",
                           "Genre", "Abbr."),
             caption = "The twenty movie genres and their abbreviations.",
             booktabs = T) %>%
  column_spec(2, border_right = T) %>%
  column_spec(4, border_right = T) %>%
  column_spec(6, border_right = T) %>%
  kable_styling(latex_options = "hold_position")
 
```

Mathematically, the model can be written as

$$
\hat{Y}_{m,u} = \sum_{g = 1}^{20}x_m^g e_g^u
$$

where $m$ is a movie, $u$ is a user, and $x_m^g$ is the genre variable described
above. Constructing the model is then equivalent to calculating the term
$e_g^u$, the effect of genre $g$ on ratings by user $u$, for each user and
genre.

I calculated the terms in two steps:

1.  For each residual rating in the training set, I gave equal fractions to each
    of the movie's genres. Thus, if a movie with genres {Ad, Co, Ro} has a
    residual rating of 0.3, I assigned a rating of 0.1 to each of the three
    genres.

2.  For each user, I gathered the fractional ratings calculated in Step 1 and
    computed the average for each genre. For example, if a user rated 3 movies
    with genres {Ad, Co} and 5 movies with genres {Co, Ro}, the Ad genre would
    get an average of 3 values, the Co genre would get an average of 8 values,
    and the Ro genre would get an average of 5 values.

The result is a matrix with 69,878 rows and 20 columns: 1 row for each user and
1 column for each genre. The rating for a movie is then computed by summing the
values in a row that correspond to the movie's genres. When I tested this, the
simple sum gave lower ratings than the actual ratings, so I introduced a tuning
factor $\lambda_2$. I then calculated the RMSE for
$\lambda_2 \cdot \hat{Y}_{m,u}$ for values of $\lambda_2$ in a range, and chose
the value of $\lambda_2$ that produced the lowest RMSE on the training set.

To compute $\hat{Y}_{m,u}$ efficiently, I noted that the sum above can be
rewritten as the dot product of the vectors $(x_m^1, \ldots , x_m^{20})$ and
$(e_1^u , \ldots , e_{20}^u )$. The first vector is the genre categorization of
movie $m$ and the second vector is the row of the above matrix for user $u$.
Therefore, multiple ratings can be calculated by computing the product of two
matrices:

-   Define $M$ to be a matrix with a row for each movie to be rated, each row
    containing $x_m^1$ through $x_m^{20}$.

-   Define $U$ to be a matrix containing with a row for each user whose ratings
    are to be calculated, each row containing $e_1^u$ through $e_{20}^u$. Row
    $i$ of $U$ is for the user whose rating is desired for the corresponding row
    $i$ of $M$.

-   The product $U$ $\cdot$ $M^\intercal$ has the average ratings on the
    diagonal. The diagonal entries are then multiplied by $\lambda_2$ to get the
    predicted ratings.

This is essential for timely calculation of ratings, since R's matrix algebra
operators are much faster than multiplying two vectors and summing the result. I
wrote code that calculates roughly 10,000 ratings at a time. Despite this
optimization, calculating the genre-based ratings took almost 600 times as long
as calculating the non-genre-based ratings.

# Results

```{r improvements}
# How much did RMSE 2 improve over RMSE 1?
improv <- function(rmse1, rmse2, sig) {
  round(100 * (1 - (rmse2 / rmse1)), sig)
}

nongenre_improv <- improv(first(nongenre_results$RMSE),
                          last(nongenre_results$RMSE),
                          1)
genre_improv <- improv(last(nongenre_results$RMSE),
                       first(combined_results$RMSE),
                       1)

```

The models using only non-genre variables all returned very similar results.
Table 4 shows each model name, the optimal tuning value $\lambda$, and the RMSE
on the training set, listed in order of descending RMSE. The following patterns
are observed:

-   *Regularizing the `year` variable does not affect accuracy:* The models
    including the year are shown in blue. The RMSE values are equal for pairs of
    models where the only difference is whether the year is regularized.

-   *Regularizing any other variable reduces accuracy:* For each pair of models
    that differ only in whether a variable is regularized, the model with the
    unregularized variable has a lower RMSE. It is a very small effect, however.

-   *The age of a movie is a better predictor than its release year: Each* model
    with the `age` variable has a lower RMSE than the corresponding model with
    the `year` variable.

-   *The best non-genre model is only slightly better than the worst non-genre
    model:* The improvement is only `r nongenre_improv`%.

```{r nongenre-model-results}
year_rows <- which(grepl("Year", nongenre_results$name))
knitr::kable(nongenre_results,
             booktabs = T,
             linesep = "",
             digits = 6,
             col.names = c("Model name", "$\\lambda$", "RMSE"),
             escape = FALSE,
             caption = "Results of non-genre models on the training set") %>%
  row_spec(year_rows, color = "blue")
```

Adding the genre effects with $\lambda_2 = 1.8$ to the best non-genre model
substantially reduced the RMSE to `r round(first(combined_results$RMSE), 5)`
(Table 5), an improvement of `r genre_improv`%. Finally, combining the two
models and running them on the validation set yielded an RMSE value of
`r round(last(combined_results$RMSE), 5)`.

```{r genre-model-results}
knitr::kable(combined_results[1, ],
             digits = 6,
             booktabs = T,
             escape = FALSE,
             col.names = c("Model name", "$\\lambda$", "$\\lambda_2$", 
                           "RMSE"),
             caption = "Result of combined models on the training set")
```

# Conclusion

The preceding analysis shows that movie ratings can be reasonably modelled by a
linear combination of user, movie, and genre variables. Determining the effect
of individual genres on a user's rating is especially important, as it allows a
rating to be predicted for a movie whose combination of genres has not been
previously rated by a user. On the other hand, including the release year or age
of the movie at rating time had very little effect on accuracy.
