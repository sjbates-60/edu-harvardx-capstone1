---
title: "Modelling Movie Ratings"
subtitle: "Report for HarvardX PH125.9x: Data Science: Capstone"
author: "Samuel Bates"
output: pdf_document
geometry: bottom=1.1in
editor_options: 
  markdown: 
    wrap: 80
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_knit$set(eval.after = "fig.cap")

require(tidyverse)
require(lubridate)
require(kableExtra)

# The following file contains the ratings and movies tables.
load("ml-10M100K/movielens_report.RData")

model_results <- readRDS("model_results.rds")

# How much did model i improve over model j?
improv <- function(i, j, sig) {
  round(100 * (1 - (model_results$RMSE[i] / model_results$RMSE[j])), sig)
}
```

# Overview

In this report, I describe my exploration of a dataset of movie ratings made by
many people over the course of fourteen years, and my construction of several
models for predicting ratings for specific movies by specific people.The models
were constructed from and tested on a training set containing 90% of the
ratings. The remaining 10% of the ratings were held out in a validation set,
which was not used until I had selected a final model among those I constructed.
I report how well the final model succeeded in predicting the ratings in the
validation set.

# Exploration

The dataset contains 10,000,054 ratings of movies by people (hereinafter
referred to as *users*). There are 10,677 different movies rated and 69,878
different users. The ratings were taken between January 9, 1995 and January 5,
2009. The dataset was provided as two files:

-   `ratings.dat`, containing 10,000,054 observations, each with 4 variables:
    `userId`, `movieId`, `rating` (a number between 0 and 5), and `timestamp`;
-   `movies.dat`, containing 10,681 observations, each with 3 variables:
    `movieId`, `title` (which contains the movie title and the year it was
    released), and `genres` (which contains a list of genres separated by `|`
    symbols).

Tables 1 and 2 show the results of counting how many movies each user rated.
Half of all users rated 69 or fewer movies, and 9 out of 10 rated 335 or fewer
movies. Every user rated at least 20 movies, and roughly 1% of the users rated
more than 1000 movies.

```{r user-stats}
percents <- ratings %>%
  group_by(userId) %>% 
  summarize(num_ratings = n()) %>% 
  pull(num_ratings) %>%
  quantile(probs = seq(0, 1, .1))
knitr::kable(t(percents), booktabs = T,
             caption = "Number of movies rated by users")

high_percents <- ratings %>% 
  group_by(userId) %>% 
  summarize(num_ratings = n()) %>% 
  pull(num_ratings) %>%
  quantile(probs = seq(.9, 1, .01))
knitr::kable(t(high_percents), booktabs = T,
             caption = "Number of movies rated by high-volume users")

rm(percents, high_percents)
```

I present three plots to motivate the use of the different variables in
modelling ratings. Figure 1 shows that there is a positive correlation between a
movie's average rating and the number of times it was rated. This makes
intuitive sense; more people will tend to go to movies that they hear have good
reviews. Figure 2 shows that there is a negative correlation between a user's
average rating and the number of movies rated by that user. One can imagine that
such a user is harder to impress, having seen so many movies.

```{r movie-correlation, fig.height=1.6, fig.cap=movie_caption}
movie_stats <- ratings %>% group_by(movieId)

mean_movie_ratings <- movie_stats %>%
  summarize(num_ratings = n(), mean_rating = mean(rating))

movie_cor <- cor(mean_movie_ratings$num_ratings, 
                 mean_movie_ratings$mean_rating)
movie_caption <-  sprintf("A movie's average rating increases as the number of ratings increases (correlation is %.4f).", movie_cor)

rm(movie_stats, movie_cor)

mean_movie_ratings %>%
  ggplot(aes(num_ratings, mean_rating)) + 
  geom_point(alpha = 0.2) + 
  scale_x_log10() + 
  xlab("Number of ratings per movie (log scale)") +
  ylab("Rating")
```

```{r user-correlation, fig.height=1.6, fig.cap=user_caption}
user_stats <- ratings %>% group_by(userId)

mean_user_ratings <- user_stats %>%
  summarize(num_ratings = n(), mean_rating = mean(rating))

user_cor <- cor(mean_user_ratings$num_ratings, mean_user_ratings$mean_rating)
user_caption <-  sprintf("A user's average movie rating decreases as the number of movies rated increases (correlation is %.4f).", user_cor)

rm(user_stats, user_cor)

mean_user_ratings %>%
  ggplot(aes(num_ratings, mean_rating)) + 
  geom_point(alpha = 0.2) + 
  scale_x_log10() +
  xlab("Number of movies rated by user (log scale)") +
  ylab("Rating")
```

Lastly, a movie's ratings may also change over time. Figure 3 shows how the
average rating per month of the movie "Jerry Maguire" decreased over the 14
years after it was released.

```{r rating-vs-time, fig.height=1.8, fig.cap="A movie's rating can change over time."}
ratings %>% 
  filter(movieId == 1393) %>%
  mutate(delay = year(timestamp) - 1996 + month(timestamp) / 13) %>%
  group_by(delay) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(delay, rating)) +
  geom_point() +
  ylim(0, 5) +
  xlab("Number of years after movie release") +
  ylab("Average rating") +
  labs(title = "Jerry Maguire")
```

# Methods and Analysis

## Data Wrangling

The `movies.dat` table has two variables that were pulled apart in order to
facilitate analysis.

-   The release year is contained in the `title` variable. This was separated
    into a new `year` variable in order to model the effect of time on ratings.

-   The `genres` variable is a list of `|`-separated genres. There are 19
    different genres, plus a "(no genre specified)" option. The variable was
    separated into 20 variables, each named for a genre. A genre variable
    contains a 1 if the movie has the variable name in its `genres` variable,
    and a 0 otherwise.

The `ratings.dat` table has a `timestamp` variable that is a second count from
12:00 am on January 1, 1970. I converted it into a `POSIXct` variable so that
the time between a movie's release year and a rating's creation time can be
calculated.

I created a training set containing 9,000,055 ratings and a validation set
containing 999,999 ratings. All exploration and modelling was done exclusively
on the training set.

My exploration was done in two phases. In the first phase, I tested models that
excluded the genres of the movies. In the second phase, I subtracted the best
model's predicted values from the actual values, and used the genres in a
subsequent model.

## Models without Genre

The models without genre that I created were all linear models on some subset of
the variables `userId`, `movieId`, `year`, and `timestamp`.[^1] I included the
user and movie variables in all of them. Rather than including the timestamp
variable directly in a model, I calculated an age variable. The age is an
approximate measure of how old the movie was when the user rated it. I
calculated it as `(year(timestamp) + month(timestamp) / 13) - year`. I divided
by 13 instead of by 12 so that the integral part of the age would equal the year
of the rating (otherwise, ratings in December would have the integral part equal
to the next year). I chose the month for the fraction rather than the day or
week of the year, as it exhibited a smoother rate of change.

[^1]: I left out `title` as I could not think of a useful distance function
    between titles.

Furthermore, I added regularization to different variables in the models. The
movie variable was always regularized, since there are many movies with 4 or
fewer ratings.

The result of these decisions was a set of ten models:

-   Two models with movie and user effects: both regularized or only movie
    effects regularized;

-   Four models with movie, user, and year effects: all three regularized and
    user or year or both effects not regularized;

-   Four models with movie, user, and age effects: all three regularized and
    user or age or both effects not regularized.

For each model, I calculated an optimal tuning parameter $\lambda$ for the
regularization and then averaged the results from 5-fold cross validation on the
training set. I then chose the model that resulted in the lowest RMSE on the
entire training set.

## Models with Genre

The second phase was to explore the effects of genre after removing the best
estimate from the first phase. I could have created another model using the
`genres` variable directly, each combination of genres being a separate
category. However, this would have been too coarse a granularity. The result
would have been that a user's rating of a comedy-romance had no relation to a
user's rating of an adventure-comedy-romance, which we intuitively know is not
likely.

Therefore, I chose a linear combination of individual genre variables for the
model. As described above, I separated the `genres` variable into 20 separate
variables, one for each of the genres in Table 3. For display purposes, I
abbreviated each genre to two characters. Each genre variable has a value of 1
if the genre appears in the movie's `genres` variable and has a value of 0
otherwise.

```{r genres-table}
all_genres <- unique(unlist(sapply(movies$genres,
                                   function(g) strsplit(g, "\\|"))))
abbrevs <- data.frame(genres = all_genres) %>%
  mutate(genres = gsub("([^\\|]{2})[^\\|]+", "\\1", genres)) %>%
  mutate(genres = sub("Sc", "SF", genres)) %>%
  mutate(genres = sub("Fi", "FN", genres)) %>%
  mutate(genres = sub("\\(n", "No", genres))

summary <- data.frame(all_genres[1:5], abbrevs[1:5, 1],
                      all_genres[6:10], abbrevs[6:10, 1],
                      all_genres[11:15], abbrevs[11:15, 1],
                      all_genres[16:20], abbrevs[16:20, 1])
knitr::kable(summary, 
             col.names = c("Genre", "Abbr.",
                           "Genre", "Abbr.",
                           "Genre", "Abbr.",
                           "Genre", "Abbr."),
             caption = "The twenty movie genres and their abbreviations.",
             booktabs = T) %>%
  column_spec(2, border_right = T) %>%
  column_spec(4, border_right = T) %>%
  column_spec(6, border_right = T) %>%
  kable_styling(latex_options = "hold_position")
 
```

Mathematically, the model can be written as

$$
\hat{Y}_{m,u} = \sum_{g = 1}^{20}x_m^g e_g^u
$$

where $m$ is a movie, $u$ is a user, and $x_m^g$ is the genre variable described
above. Constructing the model is then equivalent to calculating the term
$e_g^u$, the effect of genre $g$ on ratings by user $u$, for each user and
genre.

I calculated the terms in two steps:

1.  For each rating in the training set, I gave equal fractions to each of the
    movie's genres. Thus, if a movie with genres {Ad, Co, Ro} received a rating
    of 4.5, I assigned a rating of 1.5 to each of the three genres.

2.  For each user, I gathered the fractional ratings calculated in Step 1 and
    computed the average for each genre. For example, if a user rated 3 movies
    with genres {Ad, Co} and 5 movies with genres {Co, Ro}, the Ad genre would
    get an average of 3 values, the Co genre would get an average of 8 values,
    and the Ro genre would get an average of 5 values.

The result is a matrix with 69,878 rows and 20 columns: 1 row for each user and
1 column for each genre. The rating for a movie is then computed by summing the
values in a row that correspond to the movie's genres. When I tested this, the
simple sum gave lower ratings than the actual ratings, so I introduced a tuning
factor $\lambda_2$. I then calculated the RMSE for
$\lambda_2 \cdot \hat{Y}_{m,u}$ for values of $\lambda_2$ in a range, and chose
the value of $\lambda_2$ that produced the lowest RMSE on the training set.

To compute $\hat{Y}_{m,u}$ efficiently, I noted that the sum above can be
rewritten as the dot product of the vectors $(x_m^1, \ldots , x_m^{20})$ and
$(e_1^u , \ldots , e_{20}^u )$. The first vector is the genre categorization of
movie $m$ and the second vector is the row of the above matrix for user $u$.
Therefore, multiple ratings can be calculated by computing the product of two
matrices:

-   Define $M$ to be a matrix with a row for each movie to be rated, each row
    containing $x_m^1$ through $x_m^{20}$.

-   Define $U$ to be a matrix containing with a row for each user whose ratings
    are to be calculated, each row containing $e_1^u$ through $e_{20}^u$. Row
    $i$ of $U$ is for the user whose rating is desired for the corresponding row
    $i$ of $M$.

-   The product $U$ $\cdot$ $M^\intercal$ has the average ratings on the
    diagonal. Multiply by $\lambda_2$ to get the predicted ratings.

This is essential for timely calculation of ratings, since R's matrix algebra
operators are much faster than multiplying two vectors and summing the result. I
wrote code that calculates roughly 10,000 ratings at a time. Despite this
optimization, calculating the genre-based ratings took almost 600 times as long
as calculating the non-genre-based ratings.

# Results

The models using only non-genre variables all returned very similar results.
Table 4 shows each model name, the optimal tuning value $\lambda$, and the RMSE
on the training set. Examining the models in the pairs described in the methods
section above, I found that the model where the user effect was not regularized
performed slightly better than the modeI where the user effect was regularized.
(The pairs are indicated by the striping in Table 4.) The improvement in each
case was very small, however. The models that performed best included movie,
user, and age effects. These still showed at most a `r improv(6, 1, 1)`%
improvement over the model with only movie and user effects.

```{r nongenre-model-results}
knitr::kable(model_results[1:8, c(1,2,4)],
             booktabs = T,
             linesep = "",
             digits = 6,
             col.names = c("Model name", "$\\lambda$", "RMSE"),
             escape = FALSE,
             caption = "Results of non-genre models on the training set") %>%
  kable_styling(latex_options = c("striped"),
                stripe_index = c(1, 2, 5, 6))
```

Adding the genre effects with $\lambda_2 = 1.8$ to the best non-genre model
substantially reduced the RMSE to `r round(model_results[9, "RMSE"], 5)` (Table
5), an improvement of `r improv(9, 6, 1)`%. Finally, combining the two models
and running them on the validation set yielded an RMSE value of
`r round(model_results[10, "RMSE"], 5)`.

```{r genre-model-results}
knitr::kable(model_results[9, ],
             digits = 6,
             booktabs = T,
             escape = FALSE,
             col.names = c("Model name", "$\\lambda$", "$\\lambda_2$", 
                           "RMSE"),
             caption = "Result of combined models on the training set")
```

# Conclusion

The preceding analysis shows that movie ratings can be reasonably modelled by a
linear combination of user, movie, and genre variables. Determining the effect
of individual genres on a user's rating is especially important, as it allows a
rating to be predicted for a movie whose combination of genres has not been
previously rated by a user.

One limitation of my analysis was that I used a single tuning parameter to
regularize all variables in the non-genre models. Using a separate tuning
parameter for each regularized variable may have improved performance. My code
optimized the single tuning parameter by checking 33 values in the range [0, 8]
for each of 5 cross validation folds, and took 20 - 25 minutes per model. Each
additional tuning parameter would multiply the time by 33, so using two tuning
parameters would have increased it to between 11 and 14 hours per model, and
using three would have increased it to between 15 and 19 days per model. I
looked at the largest and smallest RMSE values found for each fold for one of
the models with three regularized variables, and discovered that they differed
by less than .005. I feel confident, therefore, that using multiple tuning
parameters would not yield any significant improvement.
